{
  "hash": "ab5a7e93a78d3cf76879975b1e5013a6",
  "result": {
    "markdown": "---\ntitle: \"An introduction to calibration with tidymodels\"\ncategories:\n  - classification\n  - calibration\ntype: learn-subsection\nweight: 6\ndescription: | \n  Learn how the probably package can improve classifcation and regression models.\ntoc: true\ntoc-depth: 2\ninclude-after-body: ../../../resources.html\n---\n\n\n\n\n\nTo use code in this article,  you will need to install the following packages: discrim, klaR, probably, and tidymodels. The probably package should be version 1.0.0 or greater. \n\nThis article demonstrates how to improve an existing model to make its predictions better. A model is well-calibrated if its probability estimate is consistent with the rate that the event occurs \"in the wild.\" In practice though, it can be difficult to validate this definition directly. \n\nFor more details: \n\n - Kull, Meelis, Telmo M. Silva Filho, and Peter Flach. \"[Beyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration.](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22Beyond+sigmoids%22+calibration&btnG=)\" (2017): 5052-5080\n\n- Niculescu-Mizil, Alexandru, and Rich Caruana. \"[Predicting good probabilities with supervised learning](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%E2%80%9CPredicting+Good+Probabilities+with+Supervised+Learning%E2%80%9D&btnG=).\" In _Proceedings of the 22nd international conference on Machine learning_, pp. 625-632. 2005.\n\nTo get started, load some packages: \n\n\n::: {.cell layout-align=\"center\" hash='cache/startup2_38d19ab3e73bef0a5620625b6e6f44b0'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(probably)\nlibrary(discrim)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\n```\n:::\n\n\nWe'll use an old example for illustration.\n\n## An example: predicting cell segmentation quality\n\nThe modeldata package contains a data set called `cells`. Initially distributed by [Hill and Haney (2007)](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-340), they showed how to create models that predict the _quality_ of the image analysis of cells. The outcome has two levels `\"PS\"` (for poorly segmented images) or `\"WS\"` (well-segmented). There are 56 image features that can be used to build a classifier. \n\nLet's load the data, remove an unwanted column, and look at the outcome frequencies: \n\n\n::: {.cell layout-align=\"center\" hash='cache/cell-data-start_fbea832d187160702cd92a7471471fd4'}\n\n```{.r .cell-code}\ndata(cells)\ncells$case <- NULL\n\ndim(cells)\n#> [1] 2019   57\ncells %>% count(class)\n#> # A tibble: 2 × 2\n#>   class     n\n#>   <fct> <int>\n#> 1 PS     1300\n#> 2 WS      719\n```\n:::\n\n\nThere is a class imbalance but that will not affect our work here. \n\nLet's make a 75%/25% split of the data into training and testing using `initial_split()`. We'll also create a set of 10-fold cross-validation indices for model resampling. \n\n\n::: {.cell layout-align=\"center\" hash='cache/cell-data-obj_e3c7d8941e70bbc216f7a234d73cb790'}\n\n```{.r .cell-code}\nset.seed(8928)\nsplit <- initial_split(cells, strata = class)\ncells_tr <- training(split)\ncells_te <- testing(split)\n\ncells_rs <- vfold_cv(cells_tr, strata = class)\n```\n:::\n\n\nNow that there are data to be modeled, let's get to it!\n\n## A naive Bayes model\n\nWe'll show the utility of calibration tools by using a model that, in this instance, is likely to produce a poorly calibrated model. The naive Bayes classifier is a well established model that assumes that the predictors are statistically _independent_ of one another (to simplify the calculations).  While that is certainly not the case for these data, the model can be effective at discriminating between the classes. Unfortunately, when there are many predictors in the model, it has a tendency to produce class probability distributions that are pathological. The predictions tend to gravitate to values neat zero or one, producing distributions that are \"U\"-shaped ([Kuhn and Johnson, 2013](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22Applied+Predictive+Modeling%22&btnG=)). \n\nTo demonstrate, let's set up the model:\n\n\n::: {.cell layout-align=\"center\" hash='cache/bayes-setup_a0304b493b0192e3c837cde365945ef5'}\n\n```{.r .cell-code}\nbayes_wflow <-\n  workflow() %>%\n  add_formula(class ~ .) %>%\n  add_model(naive_Bayes())\n```\n:::\n\n\nWe'll resample the model first so that we can get a good assessment of the results. During the resampling process, two metrics are used to judge how well the model worked. First, the area under the ROC curve is used to measure the ability of the model to separate the classes (using probability predictions). Second, the Brier score can measure how close the probability estimates are to the actual outcome values (zero or one). The `collect_metrics()` function shows the resampling estimates: \n\n\n::: {.cell layout-align=\"center\" hash='cache/bayes-resample_50f932da1d4fab54ea7988f791899e60'}\n\n```{.r .cell-code}\ncls_met <- metric_set(roc_auc, brier_class)\n# We'll save the out-of-sample predictions to visualize them. \nctrl <- control_resamples(save_pred = TRUE)\n\nbayes_res <-\n  bayes_wflow %>%\n  fit_resamples(cells_rs, metrics = cls_met, control = ctrl)\n\ncollect_metrics(bayes_res)\n#> # A tibble: 2 × 6\n#>   .metric     .estimator  mean     n std_err .config             \n#>   <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 brier_class binary     0.202    10 0.0100  Preprocessor1_Model1\n#> 2 roc_auc     binary     0.856    10 0.00936 Preprocessor1_Model1\n```\n:::\n\n\nThe ROC score is impressive! However, the Brier value indicates that the probability values, while discriminating well, are not very realistic. A value of 0.25 is the \"bad model\" threshold when there are two classes (a value of zero being the best possible result). \n\n### But is it calibrated? \n\nSpoilers: no. It is not. \n\nThe first clue is the extreme U-shaped distribution of the probability scores (facetted by the true class value): \n\n\n::: {.cell layout-align=\"center\" hash='cache/prob-hist_294f8da9abacf01e383a66d4d381b36e'}\n\n```{.r .cell-code}\ncollect_predictions(bayes_res) %>%\n  ggplot(aes(.pred_PS)) +\n  geom_histogram(col = \"white\", bins = 40) +\n  facet_wrap(~ class, ncol = 1) +\n  geom_rug(col = \"blue\", alpha = 1 / 2) + \n  labs(x = \"Probability Estimate of PS\")\n```\n\n::: {.cell-output-display}\n![](figs/prob-hist-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\nThere are almost no cells with moderate probability estimates. Furthermore, when the model is incorrect, it is \"confidently incorrect\". \n\nThe probably package has tools for visualizing and correcting model with poor calibration properties. \n\nThe most common plot is to break the predictions into about ten equally sized buckets and compute the actual event rate within each. For example, if a bin captures the samples predicted to be poorly segmented with probabilities between 20% and 30%, we should expect about a 25% event rate within that partition. Here's a plot with ten bins: \n\n\n::: {.cell layout-align=\"center\" hash='cache/break-plot_c2414d332d463c686b30b81ab8fbed47'}\n\n```{.r .cell-code}\ncal_plot_breaks(bayes_res)\n```\n\n::: {.cell-output-display}\n![](figs/break-plot-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\nThe probabilities are not showing very good accuracy. \n\nThere is also a similar function that can use moving windows with overlapping partitions. This provides a little more detail: \n\n\n::: {.cell layout-align=\"center\" hash='cache/break-windowed_7e3dbb1dbdd377e45043934a936476cc'}\n\n```{.r .cell-code}\ncal_plot_windowed(bayes_res, step_size = 0.025)\n```\n\n::: {.cell-output-display}\n![](figs/break-windowed-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\nBad. Still bad. \n\nFinally, for two class outcomes, we can fit a logistic regression model use a generalized additive model and examine the trend. \n\n\n::: {.cell layout-align=\"center\" hash='cache/break-logistic_096b3ecc8d7713660db4d9346f399823'}\n\n```{.r .cell-code}\ncal_plot_logistic(bayes_res)\n```\n\n::: {.cell-output-display}\n![](figs/break-logistic-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\nOoof. \n\n## Remediation\n\nThe good news is that we can do something about this. There are tools to fix the probability estimates so that they have better properties. \n\nThe most common approach is the fit a logistic regression model to the data (with the probability estimates as the predictor). The probability predictions from this model is then used as the calibrated estimate. By default, a generalized additive model is used for this fit, but the `smooth = FALSE` argument can use simple linear effects. \n\nHow do we know if this works? There are a set of `validate` functions that can use holdout data to resample the model with and without the calibration tool of choice. Since we already resampled the model, we'll use those results to estimate 10 more logistic regressions and use the out-of-sample data to estimate performance. \n\n`collect_metrics()` can again be used to see the performance statistics. We'll also use `cal_plot_windowed()` on the calibrated holdout data to get a visual assessment:  \n\n\n::: {.cell layout-align=\"center\" hash='cache/logistic-cal_7566bbbe417109ca39fac4bc961f7207'}\n\n```{.r .cell-code}\nlogit_val <- cal_validate_logistic(bayes_res, metrics = cls_met, save_pred = TRUE)\ncollect_metrics(logit_val)\n#> # A tibble: 4 × 7\n#>   .metric     .type        .estimator  mean     n std_err .config\n#>   <chr>       <chr>        <chr>      <dbl> <int>   <dbl> <chr>  \n#> 1 brier_class uncalibrated binary     0.202    10 0.0100  config \n#> 2 roc_auc     uncalibrated binary     0.856    10 0.00936 config \n#> 3 brier_class calibrated   binary     0.154    10 0.00608 config \n#> 4 roc_auc     calibrated   binary     0.855    10 0.00973 config\n\ncollect_predictions(logit_val) %>%\n  filter(.type == \"calibrated\") %>%\n  cal_plot_windowed(truth = class, estimate = .pred_PS, step_size = 0.025) +\n  ggtitle(\"Logistic calibration via GAM\")\n```\n\n::: {.cell-output-display}\n![](figs/logistic-cal-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\nThat's a lot better but it is problematic that the calibrated predictions to not reach zero or one. \n\nA different approach is to use isotonic regression. This method can result in very few unique probability estimates. probably has a version of isotonic regression that resamples the process to produce more unique probabilities: \n\n\n::: {.cell layout-align=\"center\" hash='cache/isoreg-cal_f898c47f6f739ea43df595ddff4c8697'}\n\n```{.r .cell-code}\nset.seed(1212)\niso_val <- cal_validate_isotonic_boot(bayes_res, metrics = cls_met, \n                                      save_pred = TRUE, times = 25)\ncollect_metrics(iso_val)\n#> # A tibble: 4 × 7\n#>   .metric     .type        .estimator  mean     n std_err .config\n#>   <chr>       <chr>        <chr>      <dbl> <int>   <dbl> <chr>  \n#> 1 brier_class uncalibrated binary     0.202    10 0.0100  config \n#> 2 roc_auc     uncalibrated binary     0.856    10 0.00936 config \n#> 3 brier_class calibrated   binary     0.150    10 0.00504 config \n#> 4 roc_auc     calibrated   binary     0.856    10 0.00928 config\n\ncollect_predictions(iso_val) %>%\n  filter(.type == \"calibrated\") %>%\n  cal_plot_windowed(truth = class, estimate = .pred_PS, step_size = 0.025) +\n  ggtitle(\"Isotonic regression calibration\")\n```\n\n::: {.cell-output-display}\n![](figs/isoreg-cal-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\nMuch better. There is a slight bias since the estimated points are consistently above the identity line on the 45 degree angle. \n\nFinally, we can also test out [Beta calibration](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22Beyond+sigmoids%22+calibration&btnG=): \n\n\n::: {.cell layout-align=\"center\" hash='cache/beta-cal_77ed70145c9b191ea0234e3525b3574d'}\n\n```{.r .cell-code}\nbeta_val <- cal_validate_beta(bayes_res, metrics = cls_met, save_pred = TRUE)\ncollect_metrics(beta_val)\n#> # A tibble: 4 × 7\n#>   .metric     .type        .estimator  mean     n std_err .config\n#>   <chr>       <chr>        <chr>      <dbl> <int>   <dbl> <chr>  \n#> 1 brier_class uncalibrated binary     0.202    10 0.0100  config \n#> 2 roc_auc     uncalibrated binary     0.856    10 0.00936 config \n#> 3 brier_class calibrated   binary     0.145    10 0.00439 config \n#> 4 roc_auc     calibrated   binary     0.856    10 0.00933 config\n\ncollect_predictions(beta_val) %>%\n  filter(.type == \"calibrated\") %>%\n  cal_plot_windowed(truth = class, estimate = .pred_PS, step_size = 0.025) +\n  ggtitle(\"Beta calibration\")\n```\n\n::: {.cell-output-display}\n![](figs/beta-cal-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\nAlso a big improvement but it does poorly at the lower end of the scale. \n\nIsotonic regression appears to have the best results. We'll save a model that is trained using all of the out-of-sample predictions from the original naive Bayes resampling results. The `cell_cal` object can be used to enact the calibration for new predictions. \n\nWe can also fit the final naive Bayes model to predict the test set: \n\n\n::: {.cell layout-align=\"center\" hash='cache/finalize_ec5b34af844db283107c7b6f8d183c6d'}\n\n```{.r .cell-code}\ncell_cal <- cal_estimate_isotonic(bayes_res, times = 25)\nbayes_fit <- bayes_wflow %>% fit(data = cells_tr)\n```\n:::\n\n\n## Test set results\n\nFirst, we make our ordinary predictions: \n\n\n::: {.cell layout-align=\"center\" hash='cache/uncalibrated_8c8b0e4e6ec6978e7cd9de94812bcd66'}\n\n```{.r .cell-code}\ncell_test_pred <- augment(bayes_fit, new_data = cells_te)\ncell_test_pred %>% cls_met(class, .pred_PS)\n#> # A tibble: 2 × 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 roc_auc     binary         0.839\n#> 2 brier_class binary         0.226\n```\n:::\n\n\nThese metric estimates are very consistent with the resasmpled performance estimates. \n\nWe can then use our `cell_cal` object with the `cal_apply()` function:\n\n\n::: {.cell layout-align=\"center\" hash='cache/calibrated_6037e02a294b46dd665076034ea8570b'}\n\n```{.r .cell-code}\ncell_test_cal_pred <-\n  cell_test_pred %>%\n  cal_apply(cell_cal)\ncell_test_cal_pred %>% dplyr::select(class, starts_with(\".pred_\"))\n#> # A tibble: 505 × 4\n#>    class .pred_class .pred_PS .pred_WS\n#>    <fct> <fct>          <dbl>    <dbl>\n#>  1 PS    PS            0.848    0.152 \n#>  2 WS    WS            0.137    0.863 \n#>  3 WS    WS            0.0290   0.971 \n#>  4 PS    PS            0.791    0.209 \n#>  5 PS    PS            0.921    0.0790\n#>  6 WS    WS            0.0883   0.912 \n#>  7 PS    PS            0.800    0.200 \n#>  8 PS    PS            0.673    0.327 \n#>  9 WS    WS            0.235    0.765 \n#> 10 WS    PS            0.516    0.484 \n#> # ℹ 495 more rows\n```\n:::\n\n\nNote that `cal_apply()` recomputed the hard class predictions in the `.pred_class` column. It is possible that the changes in the probability estimates could invalidate the original hard class estimates. \n\nWhat do the calibrated test set results show? \n\n\n::: {.cell layout-align=\"center\" hash='cache/calibrated-res_0f60947206a5f6f3bc4f05b3520806e0'}\n\n```{.r .cell-code}\ncell_test_cal_pred %>% cls_met(class, .pred_PS)\n#> # A tibble: 2 × 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 roc_auc     binary         0.839\n#> 2 brier_class binary         0.160\ncell_test_cal_pred %>%\n  cal_plot_windowed(truth = class, estimate = .pred_PS, step_size = 0.025)\n```\n\n::: {.cell-output-display}\n![](figs/calibrated-res-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\nMuch better. The test set results also agree with the results from `cal_validate_isotonic_boot().` \n\n## Other model types\n\nprobably can also calibrate classification models with more than two outcome levels. The functions `cal_*_multinomial()` uses a multinomial model in the same spirit as the logistic regression model. Also, isotonic and Beta calibration can also be used via a \"one versus all\" approach that builds a set of binary calibrators and normalizes their results at the end (to ensure that they add to one). \n\nFor regression models, there is `cal_plot_regression()` and `cal_*_linear()`. The latter uses `lm()` or `mgcv::gam()` to create a calibrator object. \n\n## Future plans\n\nThe tidymodels group is currently working on adding post-processors to workflow objects. This will allow a model workflow to modify the predictions of a model. Calibration is an important feature for post-processing. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}