{
  "hash": "f228cf2a5363ad22f49d4cf8015536f2",
  "result": {
    "markdown": "---\ntitle: \"Classification models using a neural network\"\ntags: [rsample, parsnip]\ncategories: [model fitting]\ntype: learn-subsection\nweight: 2\ndescription: | \n  Train a classification model and evaluate its performance.\n---\n\n\n\n\n\n\n\n## Introduction\n\nTo use code in this article,  you will need to install the following packages: brulee and tidymodels. You will also need the python keras library installed (see `?keras::install_keras()`).\n\nWe can create classification models with the tidymodels package [parsnip](https://parsnip.tidymodels.org/) to predict categorical quantities or class labels. Here, let's fit a single classification model using a neural network and evaluate using a validation set. While the [tune](https://tune.tidymodels.org/) package has functionality to also do this, the parsnip package is the center of attention in this article so that we can better understand its usage. \n\n## Fitting a neural network\n\n\nLet's fit a model to a small, two predictor classification data set. The data are in the modeldata package (part of tidymodels) and have been split into training, validation, and test data sets. In this analysis, the test set is left untouched; this article tries to emulate a good data usage methodology where the test set would only be evaluated once at the end after a variety of models have been considered. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(bivariate)\nnrow(bivariate_train)\n#> [1] 1009\nnrow(bivariate_val)\n#> [1] 300\n```\n:::\n\n\nA plot of the data shows two right-skewed predictors: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(bivariate_train, aes(x = A, y = B, col = Class)) + \n  geom_point(alpha = .2)\n```\n\n::: {.cell-output-display}\n![](figs/biv-plot-1.svg){fig-align='center' width=576}\n:::\n:::\n\n\nLet's use a single hidden layer neural network to predict the outcome. To do this, we transform the predictor columns to be more symmetric (via the `step_BoxCox()` function) and on a common scale (using `step_normalize()`). We can use [recipes](https://recipes.tidymodels.org/) to do so:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbiv_rec <- \n  recipe(Class ~ ., data = bivariate_train) %>%\n  step_BoxCox(all_predictors())%>%\n  step_normalize(all_predictors()) %>%\n  prep(training = bivariate_train, retain = TRUE)\n\n# We will bake(new_data = NULL) to get the processed training set back\n\n# For validation:\nval_normalized <- bake(biv_rec, new_data = bivariate_val, all_predictors())\n# For testing when we arrive at a final model: \ntest_normalized <- bake(biv_rec, new_data = bivariate_test, all_predictors())\n```\n:::\n\n\nWe can use the keras package to fit a model with 5 hidden units and a 10% dropout rate, to regularize the model:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(57974)\nnnet_fit <-\n  mlp(epochs = 100, hidden_units = 5, dropout = 0.1) %>%\n  set_mode(\"classification\") %>% \n  set_engine(\"brulee\") %>%\n  fit(Class ~ ., data = bake(biv_rec, new_data = NULL))\n\nnnet_fit\n#> parsnip model object\n#> \n#> Multilayer perceptron\n#> \n#> relu activation\n#> 5 hidden units,  27 model parameters\n#> 1,009 samples, 2 features, 2 classes \n#> class weights One=1, Two=1 \n#> weight decay: 0.001 \n#> dropout proportion: 0.1 \n#> batch size: 909 \n#> learn rate: 0.01 \n#> validation loss after 23 epochs: 0.704\n```\n:::\n\n\n## Model performance\n\nIn parsnip, the `predict()` function can be used to characterize performance on the validation set. Since parsnip always produces tibble outputs, these can just be column bound to the original data: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nval_results <- \n  bivariate_val %>%\n  bind_cols(\n    predict(nnet_fit, new_data = val_normalized),\n    predict(nnet_fit, new_data = val_normalized, type = \"prob\")\n  )\nval_results %>% slice(1:5)\n#> # A tibble: 5 × 6\n#>       A     B Class .pred_class .pred_One .pred_Two\n#>   <dbl> <dbl> <fct> <fct>           <dbl>     <dbl>\n#> 1 1061.  74.5 One   Two             0.451     0.549\n#> 2 1241.  83.4 One   Two             0.481     0.519\n#> 3  939.  71.9 One   Two             0.431     0.569\n#> 4  813.  77.1 One   Two             0.418     0.582\n#> 5 1706.  92.8 Two   Two             0.494     0.506\n\nval_results %>% roc_auc(truth = Class, .pred_One)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.448\n\nval_results %>% accuracy(truth = Class, .pred_class)\n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.327\n\nval_results %>% conf_mat(truth = Class, .pred_class)\n#>           Truth\n#> Prediction One Two\n#>        One   0   0\n#>        Two 202  98\n```\n:::\n\n\nLet's also create a grid to get a visual sense of the class boundary for the validation set.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\na_rng <- range(bivariate_train$A)\nb_rng <- range(bivariate_train$B)\nx_grid <-\n  expand.grid(A = seq(a_rng[1], a_rng[2], length.out = 100),\n              B = seq(b_rng[1], b_rng[2], length.out = 100))\nx_grid_trans <- bake(biv_rec, x_grid)\n\n# Make predictions using the transformed predictors but \n# attach them to the predictors in the original units: \nx_grid <- \n  x_grid %>% \n  bind_cols(predict(nnet_fit, x_grid_trans, type = \"prob\"))\n\nggplot(x_grid, aes(x = A, y = B)) + \n  geom_contour(aes(z = .pred_One), breaks = .5, col = \"black\") + \n  geom_point(data = bivariate_val, aes(col = Class), alpha = 0.3)\n```\n\n::: {.cell-output-display}\n![](figs/biv-boundary-1.svg){fig-align='center' width=576}\n:::\n:::\n\n\n\n\n## Session information\n\n\n::: {.cell layout-align=\"center\"}\n\n```\n#> ─ Session info ─────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.0 (2022-04-22)\n#>  os       macOS Big Sur/Monterey 10.16\n#>  system   x86_64, darwin17.0\n#>  ui       X11\n#>  language (EN)\n#>  collate  en_US.UTF-8\n#>  ctype    en_US.UTF-8\n#>  tz       America/New_York\n#>  date     2023-01-03\n#>  pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#> \n#> ─ Packages ─────────────────────────────────────────────────────────\n#>  package    * version    date (UTC) lib source\n#>  broom      * 1.0.1      2022-08-29 [1] CRAN (R 4.2.0)\n#>  brulee       0.2.0      2022-09-19 [1] CRAN (R 4.2.0)\n#>  dials      * 1.1.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  dplyr      * 1.0.10     2022-09-01 [1] CRAN (R 4.2.0)\n#>  ggplot2    * 3.4.0      2022-11-04 [1] CRAN (R 4.2.0)\n#>  infer      * 1.0.3      2022-08-22 [1] CRAN (R 4.2.0)\n#>  parsnip    * 1.0.3      2022-11-11 [1] CRAN (R 4.2.0)\n#>  purrr      * 0.3.5      2022-10-06 [1] CRAN (R 4.2.0)\n#>  recipes    * 1.0.3.9000 2022-12-12 [1] local\n#>  rlang        1.0.6      2022-09-24 [1] CRAN (R 4.2.0)\n#>  rsample    * 1.1.1.9000 2022-12-13 [1] local\n#>  tibble     * 3.1.8      2022-07-22 [1] CRAN (R 4.2.0)\n#>  tidymodels * 1.0.0      2022-07-13 [1] CRAN (R 4.2.0)\n#>  tune       * 1.0.1.9001 2022-12-05 [1] Github (tidymodels/tune@e23abdf)\n#>  workflows  * 1.1.2      2022-11-16 [1] CRAN (R 4.2.0)\n#>  yardstick  * 1.1.0.9000 2022-11-30 [1] Github (tidymodels/yardstick@5f1b9ce)\n#> \n#>  [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n#> \n#> ────────────────────────────────────────────────────────────────────\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}